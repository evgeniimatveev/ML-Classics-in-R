# ğŸ“ The Regression Formula:
# Å· = b0 + b1*X + b2*XÂ² + ... + bn*Xâ¿ + Ïµ

# ğŸ“Œ Where:
# âœ… Å· (Predicted Value) â†’ The estimated outcome (e.g., salary ğŸ’°)
# âœ… X, XÂ², ..., Xâ¿ (Polynomial Features) â†’ Transformed input features capturing non-linearity ğŸ“Š
# âœ… b0 (Intercept) â³ â†’ The base salary when X = 0
# âœ… b1, b2, ..., bn (Coefficients) ğŸ“ˆ â†’ The rate of change for each polynomial term of X
# âœ… Ïµ (Error Term) âš ï¸ â†’ Represents the difference between actual and predicted values due to noise

# ğŸ’¡ Key Insights:
# - The intercept (b0) represents the base salary when Level = 0.
# - Each coefficient (b1, b2, ..., bn) determines how much the predicted salary changes.
# - If b1, b2, ..., bn are **positive**, the polynomial curve trends **upward** ğŸ“ˆ.
# - If b1, b2, ..., bn are **negative**, the polynomial curve trends **downward** ğŸ“‰.
# - Unlike **Multiple Linear Regression**, Polynomial Regression captures **non-linear** relationships.

# -----------------------------------
# ğŸ“¦ Step 1: Load necessary libraries
# -----------------------------------
library(ggplot2)  # ğŸ“Š For visualization

# -----------------------------------
# ğŸ“‚ Step 2: Load the dataset
# -----------------------------------
dataset <- read.csv("Position_Salaries.csv")  # Load data from CSV file
dataset <- dataset[2:3]  # Select relevant columns: Level (X) and Salary (y)

# -----------------------------------
# ğŸ“ˆ Step 3: Train Linear Regression Model
# -----------------------------------
# Formula:  Salary = Î²0 + Î²1 * Level  (Simple Linear Regression)
lin_reg <- lm(Salary ~ Level, data = dataset)  # Train a linear regression model

# -----------------------------------
# ğŸ”„ Step 4: Transform dataset for Polynomial Regression (Degree 4)
# -----------------------------------
# Polynomial Regression Formula (Degree 4):
# Salary = Î²0 + Î²1 * Level + Î²2 * LevelÂ² + Î²3 * LevelÂ³ + Î²4 * Levelâ´ + Îµ
dataset$Level2 <- dataset$Level^2  # Generate LevelÂ²
dataset$Level3 <- dataset$Level^3  # Generate LevelÂ³
dataset$Level4 <- dataset$Level^4  # Generate Levelâ´

# Train the polynomial regression model
poly_reg <- lm(Salary ~ Level + Level2 + Level3 + Level4, data = dataset)

# -----------------------------------
# ğŸ“Š Step 5: Visualizing the Linear Regression Model
# -----------------------------------
ggplot() +
  geom_point(aes(x = dataset$Level, y = dataset$Salary), color = "red") +  # Actual data points
  geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)), color = "blue") + 
  ggtitle("Truth or Bluff (Linear Regression)") +
  xlab("Level") + ylab("Salary")

# -----------------------------------
# ğŸ“Š Step 6: Visualizing the Polynomial Regression Model (Smooth Curve)
# -----------------------------------
# ğŸ”¹ We use a finer grid of x-values for a smooth curve
x_grid <- seq(min(dataset$Level), max(dataset$Level), 0.1)  # Generate high-resolution X-axis
dataset_grid <- data.frame(Level = x_grid, Level2 = x_grid^2, Level3 = x_grid^3, Level4 = x_grid^4)

ggplot() +
  geom_point(aes(x = dataset$Level, y = dataset$Salary), color = "red") +  # Real data points
  geom_line(aes(x = x_grid, y = predict(poly_reg, newdata = dataset_grid)), color = "blue") + 
  ggtitle("Truth or Bluff (Polynomial Regression)") +
  xlab("Level") + ylab("Salary")


# -----------------------------------
# ğŸ“Š Step 8: Additional Visualization - Comparison of Linear vs Polynomial Predictions
# -----------------------------------
ggplot() +
  geom_point(aes(x = dataset$Level, y = dataset$Salary), color = "red") +  # Actual Data
  geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)), color = "blue", linetype="dashed") +  # Linear Prediction
  geom_line(aes(x = x_grid, y = predict(poly_reg, newdata = dataset_grid)), color = "green") +  # Polynomial Prediction
  ggtitle("Comparison: Linear vs Polynomial Regression") +
  xlab("Level") + ylab("Salary") +
  theme_minimal()

# -----------------------------------
# ğŸ”® Step 9: Predicting Salary for Level = 6.5
# -----------------------------------
# Prepare new data for prediction
new_data <- data.frame(Level = 6.5, Level2 = 6.5^2, Level3 = 6.5^3, Level4 = 6.5^4)

# ğŸ”¹ Predict salary using Polynomial Regression
predicted_salary <- predict(poly_reg, newdata = new_data)
cat("ğŸ”® Predicted Salary for Level 6.5:", predicted_salary, "\n")

# -----------------------------------
# ğŸ† Step 10: Model Evaluation - Comparing Linear vs Polynomial Regression
# -----------------------------------
# ğŸ“Š We will compute key regression metrics:
# âœ… RÂ² Score (coefficient of determination)
# âœ… MAE (Mean Absolute Error)
# âœ… MSE (Mean Squared Error)
# âœ… RMSE (Root Mean Squared Error)

# ğŸ“Œ Compute RÂ² Score for both models
r2_lin <- summary(lin_reg)$r.squared
r2_poly <- summary(poly_reg)$r.squared

# ğŸ“Œ Compute MAE, MSE, and RMSE
mae <- function(actual, predicted) { mean(abs(actual - predicted)) }  # MAE function

y_actual <- dataset$Salary  # True salaries
y_pred_lin <- predict(lin_reg, newdata = dataset)  # Predictions from Linear Regression
y_pred_poly <- predict(poly_reg, newdata = dataset)  # Predictions from Polynomial Regression

mae_lin <- mae(y_actual, y_pred_lin)
mae_poly <- mae(y_actual, y_pred_poly)

mse_lin <- mean((y_actual - y_pred_lin)^2)
mse_poly <- mean((y_actual - y_pred_poly)^2)

rmse_lin <- sqrt(mse_lin)
rmse_poly <- sqrt(mse_poly)

# ğŸ“Š Create a dataframe for structured comparison
performance_comparison <- data.frame(
  Model = c("Linear Regression", "Polynomial Regression (Degree=4)"),
  R2_Score = c(r2_lin, r2_poly),
  MAE = c(mae_lin, mae_poly),
  MSE = c(mse_lin, mse_poly),
  RMSE = c(rmse_lin, rmse_poly)
)

# ğŸ“Š Display the performance comparison table
print(performance_comparison)

# -----------------------------------
# ğŸ“¢ Step 11: Summary & Conclusion
# -----------------------------------
cat("\nğŸ† **Final Conclusion:** Polynomial Regression (Degree=4) significantly outperforms Linear Regression. ğŸ¯\n")
cat("ğŸ“Œ **RÂ² Score:** Polynomial = High accuracy (", round(r2_poly, 4), ") vs. Linear = Poor fit (", round(r2_lin, 4), ").\n")
cat("ğŸ“Œ **Mean Absolute Error (MAE):** Polynomial is ~", round(mae_lin/mae_poly, 1), "x smaller, meaning more precise predictions.\n")
cat("ğŸ“Œ **RMSE:** Polynomial = ", round(rmse_poly, 2), ", Linear = ", round(rmse_lin, 2), ", showing a much better fit.\n")
cat("\nâœ… **Polynomial Regression is the clear winner for this dataset!** ğŸš€\n")


# -----------------------------------
# ğŸ“Š Step 12: Residual Plot for Linear Regression
# -----------------------------------
# ğŸ”¹ Residuals = Actual - Predicted
residuals_lin <- dataset$Salary - predict(lin_reg, newdata = dataset)

ggplot(data.frame(Level = dataset$Level, Residuals = residuals_lin)) +
  geom_point(aes(x = Level, y = Residuals), color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  ggtitle("Residuals Plot: Linear Regression") +
  xlab("Level") + ylab("Residuals") +
  theme_minimal()

# -----------------------------------
# ğŸ“Š Step 13: Residual Plot for Polynomial Regression
# -----------------------------------
residuals_poly <- dataset$Salary - predict(poly_reg, newdata = dataset)

ggplot(data.frame(Level = dataset$Level, Residuals = residuals_poly)) +
  geom_point(aes(x = Level, y = Residuals), color = "green") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  ggtitle("Residuals Plot: Polynomial Regression") +
  xlab("Level") + ylab("Residuals") +
  theme_minimal()

# -----------------------------------
# ğŸ“Š Step 14: Comparison of Errors Between Models
# -----------------------------------
# ğŸ”¹ Create a dataframe to compare model errors
error_comparison <- data.frame(
  Model = c("Linear Regression", "Polynomial Regression (Degree=4)"),
  MAE = c(mae_lin, mae_poly),
  RMSE = c(rmse_lin, rmse_poly)
)

# ğŸ”¹ Visualizing the error comparison
library(reshape2)  # Needed for reshaping data
error_comparison_melted <- melt(error_comparison, id.vars = "Model")

ggplot(error_comparison_melted, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Comparison of Errors: Linear vs Polynomial Regression") +
  xlab("Model") + ylab("Error Value") +
  scale_fill_manual(values = c("MAE" = "orange", "RMSE" = "blue")) +
  theme_minimal()